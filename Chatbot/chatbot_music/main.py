import os # Th∆∞ vi·ªán h·ªá th·ªëng c√≥ t√°c d√πng t∆∞∆°ng t√°c v·ªõi h·ªá th·ªëng m√°y t√≠nh (D√πng ƒë·ªÉ ƒë·ªçc file csv, l∆∞u file cache, l∆∞u file vector_store)
import json # Th∆∞ vi·ªán x·ª≠ l√Ω JSON
import re # Th∆∞ vi·ªán x·ª≠ l√Ω chu·ªói (D√πng ƒë·ªÉ tr√≠ch xu·∫•t link t·ª´ c√¢u tr·∫£ l·ªùi)
import streamlit as st # Th∆∞ vi·ªán t·∫°o giao di·ªán web (Thay v√¨ c√°c b·∫°n code html, css...)
import google.generativeai as genai # Th∆∞ vi·ªán t∆∞∆°ng t√°c v·ªõi API Gemini
import pandas as pd # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng (D√πng ƒë·ªÉ ƒë·ªçc file csv)
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from dotenv import load_dotenv

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
CACHE_FILE = "user_cache.json"

# ----------------------------- Rag -----------------------------
# H√†m kh·ªüi t·∫°o embedding
def initialize_embedding(model_name: str = "sentence-transformers/all-MiniLM-L12-v2"):
    return HuggingFaceEmbeddings(model_name=model_name)

# H√†m t·∫°o vector search (ƒê·ªÉ t√¨m ki·∫øm t∆∞∆°ng t·ª±, t√¨m ki·∫øm vector)
def create_vector_search(data, embedding_function, distance_strategy=DistanceStrategy.COSINE):
    return FAISS.from_documents(data, embedding_function, distance_strategy=distance_strategy)

# H√†m t√¨m ki·∫øm t∆∞∆°ng t·ª±
def similarity_search(vector_store, question: str, k: int = 1):
    retrieved_docs = vector_store.similarity_search(question, k=k)
    context = "".join(doc.page_content + "\n" for doc in retrieved_docs)
    return context

# H√†m t·∫£i d·ªØ li·ªáu t·ª´ file csv 
def load_csv_as_documents(file_path):
    """Load CSV v√† chuy·ªÉn ƒë·ªïi th√†nh danh s√°ch Document."""
    data = pd.read_csv(file_path)
    documents = [
        Document(
            page_content=(
                f"Song Title: {row['Song_Title']}, "
                f"Artist: {row['Artist']}, "
                f"Genre: {row['Genre']}, "
                f"Mood: {row['Mood']}, "
                f"Situation: {row['Situation']}, "
                f"Energy Level: {row['Energy_Level']}, "
                f"Release Year: {row['Release_Year']}, "
                f"Link: {row['Link_URL']}"
            ),
            metadata={"Song_ID": row["Song_ID"]}
        )
        for _, row in data.iterrows()
    ]
    return documents

# H√†m l∆∞u vector store v√†o local
def save_vector_store_to_local(vector_store, output_dir):
    try:
        vector_store.save_local(output_dir)
        print(f"‚úÖ FAISS vector database saved to: {output_dir}")
    except Exception as e:
        print(f"‚ùå Error saving vector store: {e}")
        raise

# H√†m t·∫£i vector store t·ª´ local
def load_vector_store_from_local(vector_store_path, embedding_function):
    try:
        vector_store = FAISS.load_local(
            vector_store_path, embeddings=embedding_function, allow_dangerous_deserialization=True
        )
        print(f"‚úÖ FAISS vector database loaded from: {vector_store_path}")
        return vector_store
    except Exception as e:
        print(f"‚ùå Error loading vector store: {e}")
        raise

# ----------------------------- Cache -----------------------------
# H√†m l∆∞u th√¥ng tin ng∆∞·ªùi d√πng v√†o file cache
def save_user_to_cache(user_data):
    """L∆∞u th√¥ng tin ng∆∞·ªùi d√πng v√†o file cache."""
    with open(CACHE_FILE, "w") as f:
        json.dump(user_data, f)

# H√†m t·∫£i th√¥ng tin ng∆∞·ªùi d√πng t·ª´ file cache
def load_user_from_cache():
    """T·∫£i th√¥ng tin ng∆∞·ªùi d√πng t·ª´ file cache."""
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r") as f:
            return json.load(f)
    return None

# H√†m th√™m c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o cache
def add_query_to_cache(user_query):
    """Th√™m m·ªôt c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o danh s√°ch cache."""
    cache_data = load_user_from_cache() or {}
    if "user_queries" not in cache_data:
        cache_data["user_queries"] = []
    cache_data["user_queries"].append(user_query)
    with open(CACHE_FILE, "w") as f:
        json.dump(cache_data, f)

# ----------------------------- Gemini API -----------------------------
# C·∫•u h√¨nh setup model
def configure_gemini_api(model_id: str = "gemini-1.5-pro"):
    """C·∫•u h√¨nh API Gemini v√† ki·ªÉm tra kh√≥a API."""
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY is not set. Check your .env file.")
    
    genai.configure(api_key=GEMINI_API_KEY)
    print("Gemini API configured successfully.")
    
    generation_config = {
        "temperature": 0,
        "top_p": 0.95,
        "top_k": 64,
        "max_output_tokens": 8192,
        "response_mime_type": "text/plain",
    }
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]

    system_instruction = (
        "B·∫°n l√† MusicMate, m·ªôt tr·ª£ l√Ω √¢m nh·∫°c th√¥ng minh. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi b·∫±ng ti·∫øng vi·ªát h·ªó tr·ª£ ng∆∞·ªùi d√πng t√¨m ki·∫øm b√†i h√°t, g·ª£i √Ω playlist, "
        "v√† cung c·∫•p th√¥ng tin li√™n quan ƒë·∫øn √¢m nh·∫°c. H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi r√µ r√†ng, chi ti·∫øt v√† ph√π h·ª£p v·ªõi s·ªü th√≠ch, t√¨nh hu·ªëng nghe nh·∫°c, "
        "v√† t√¢m tr·∫°ng c·ªßa ng∆∞·ªùi d√πng. N·∫øu c·∫ßn, h√£y g·ª£i √Ω c√°c b√†i h√°t ho·∫∑c playlist c·ª• th·ªÉ ƒë·ªÉ ng∆∞·ªùi d√πng c√≥ tr·∫£i nghi·ªám √¢m nh·∫°c t·ªët nh·∫•t."
    )

    model = genai.GenerativeModel(
        model_name=model_id,
        safety_settings=safety_settings,
        generation_config=generation_config,
        system_instruction=system_instruction
    )
    
    return model

# Nh·∫≠n request t·ª´ ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi
def generate_response(question: str, model, context: str = None):
    if not context:
        prompt = f"Question: {question}"
    else:
        prompt = f"Context: {context}\nQuestion: {question}"

    chat_session = model.start_chat(history=[])
    response = chat_session.send_message(prompt)

    return response.text

# ----------------------------- Giao di·ªán Web -----------------------------
# H√†m c·∫•u h√¨nh trang web
def setup_page():
    st.set_page_config(
        page_title="MusicMate",
        page_icon="üé∂",
        layout="centered"
    )

# H√†m hi·ªÉn th·ªã ti√™u ƒë·ªÅ
def display_title():
    st.title("üé∂ MusicMate")
    st.subheader("Ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh √¢m nh·∫°c c·ªßa b·∫°n!")

# H√†m thu th·∫≠p th√¥ng tin ng∆∞·ªùi d√πng
def gather_user_info():
    st.write(
        "Ch√†o b·∫°n! M√¨nh l√† **MusicMate**, ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh √¢m nh·∫°c c·ªßa b·∫°n. "
        "M√¨nh s·∫Ω gi√∫p b·∫°n t√¨m nh·ªØng b√†i h√°t v√† playlist tuy·ªát v·ªùi ƒë·ªÉ th∆∞ gi√£n, l√†m vi·ªác, hay n√¢ng cao t√¢m tr·∫°ng!"
    )
    st.write("Tr∆∞·ªõc ti√™n, m√¨nh mu·ªën bi·∫øt m·ªôt ch√∫t th√¥ng tin v·ªÅ b·∫°n ƒë·ªÉ ƒë·ªÅ xu·∫•t nh·∫°c ph√π h·ª£p nh√©!")

    user_name = st.text_input("B·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt t√™n c·ªßa b·∫°n ƒë∆∞·ª£c kh√¥ng?", placeholder="Nh·∫≠p t√™n c·ªßa b·∫°n")
    favorite_genre = st.text_input(
        "B·∫°n th√≠ch th·ªÉ lo·∫°i nh·∫°c n√†o? (V√≠ d·ª•: Pop, Rock, EDM, Hiphop, Ballad, v.v.)",
        placeholder="Nh·∫≠p th·ªÉ lo·∫°i nh·∫°c b·∫°n th√≠ch"
    )
    listening_context = st.text_input(
        "B·∫°n th∆∞·ªùng nghe nh·∫°c trong nh·ªØng t√¨nh hu·ªëng n√†o? (L√†m vi·ªác, th∆∞ gi√£n, t·∫≠p th·ªÉ d·ª•c, v.v.)",
        placeholder="Nh·∫≠p t√¨nh hu·ªëng b·∫°n th∆∞·ªùng nghe nh·∫°c"
    )
    user_mood = st.text_input(
        "T√¢m tr·∫°ng hi·ªán t·∫°i c·ªßa b·∫°n l√† g√¨? (Vui v·∫ª, m·ªát m·ªèi, cƒÉng th·∫≥ng, v.v.)",
        placeholder="Nh·∫≠p t√¢m tr·∫°ng hi·ªán t·∫°i c·ªßa b·∫°n"
    )
    if st.button("G·ª≠i th√¥ng tin"):
        if user_name:
            user_data = {
                "name": user_name,
                "favorite_genre": favorite_genre,
                "listening_context": listening_context,
                "user_mood": user_mood
            }
            save_user_to_cache(user_data) 
            st.session_state["chat_started"] = True
            st.success("Th√¥ng tin c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!")
        else:
            st.error("Vui l√≤ng nh·∫≠p t√™n c·ªßa b·∫°n tr∆∞·ªõc khi g·ª≠i th√¥ng tin.")

# H√†m kh·ªüi t·∫°o l·ªãch s·ª≠ chat
def initialize_chat_history():
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.audio = []

# H√†m hi·ªÉn th·ªã tin nh·∫Øn chat
def display_chat_messages():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# H√†m tr√≠ch xu·∫•t v√† ph√°t c√°c link nh·∫°c t·ª´ c√¢u tr·∫£ l·ªùi
def extract_and_play_links(response):
    links = re.findall(r'https?://\S+', response)

    if links:
        for link in links:
            if "soundcloud.com" in link:
                # Nh√∫ng tr√¨nh ph√°t b·∫±ng iframe
                embed_link = link.replace("https://soundcloud.com", "https://w.soundcloud.com/player/?url=https://soundcloud.com")
                iframe_html = f"""
                <iframe width="100%" height="166" scrolling="no" frameborder="no" 
                        allow="autoplay" 
                        src="{embed_link}">
                </iframe>
                """
                st.markdown(iframe_html, unsafe_allow_html=True)

# H√†m x·ª≠ l√Ω input t·ª´ ng∆∞·ªùi d√πng
def handle_user_input(cached_user, user_input, model, vector_store):
    st.session_state.messages.append({"role": "user", "content": user_input})
    add_query_to_cache(user_input)
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        with st.spinner("ƒêang suy nghƒ©..."):
            personalized_context = ""
            if cached_user:
                personalized_context = (
                    f"T√™n: {cached_user.get('name', 'Ng∆∞·ªùi d√πng')}\n"
                    f"Th·ªÉ lo·∫°i nh·∫°c y√™u th√≠ch: {cached_user.get('favorite_genre', 'Ch∆∞a r√µ')}\n"
                    f"T√¢m tr·∫°ng: {cached_user.get('user_mood', 'Ch∆∞a r√µ')}\n"
                    f"T√¨nh hu·ªëng nghe nh·∫°c: {cached_user.get('listening_context', 'Ch∆∞a r√µ')}\n"
                )

            context_from_vector_store = similarity_search(vector_store, user_input, k=10)

            print(context_from_vector_store)

            restricted_prompt = (
                f"Th√¥ng tin c√° nh√¢n h√≥a:\n{personalized_context}\n\n"
                f"Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung ph√π h·ª£p sau ƒë√¢y:\n{context_from_vector_store}\n"
                f"Nghe t·∫°i ƒë√¢y: [Link] n·∫øu c√≥.\n"
                f"N·∫øu n·ªôi dung kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p, h√£y tr·∫£ l·ªùi d·ª±a tr√™n hi·ªÉu bi·∫øt th√¥ng th∆∞·ªùng:\n"
                f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_input}"
            )

            assistant_response = generate_response(user_input, model, restricted_prompt)
            extract_and_play_links(assistant_response)
            st.markdown(assistant_response)

    st.session_state.messages.append({"role": "assistant", "content": assistant_response})
            
# ----------------------------- Main Function -----------------------------
def main():
    setup_page()
    display_title()
    cached_user = load_user_from_cache()

    if not cached_user or not cached_user.get("name"):
        gather_user_info()
    else:
        initialize_chat_history()
        display_chat_messages()
        model = configure_gemini_api("gemini-1.5-pro")
        embedding_function = initialize_embedding()
        vector_store_path = "vector_store"

        if not os.path.exists(os.path.join(vector_store_path, "index.faiss")):
            csv_file = "songs.csv"
            if not os.path.exists(csv_file):
                st.error("File songs.csv kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i.")
                return
            documents = load_csv_as_documents(csv_file)
            vector_store = create_vector_search(documents, embedding_function)
            save_vector_store_to_local(vector_store, vector_store_path)
        else:
            vector_store = load_vector_store_from_local(vector_store_path, embedding_function)
        if user_input := st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n?"):
            handle_user_input(cached_user, user_input, model, vector_store)
if __name__ == "__main__":
    main()
